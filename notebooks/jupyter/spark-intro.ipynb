{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Spark Intro: Cross Join Customers and Products with Business Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, I want to share with you my aha moments I had during development of my first (Py) Spark application. \n",
    "\n",
    "We do this by an example application: \n",
    "* Read **customers**, **products** and **transactions**\n",
    "* Create the **cross join** of customers and products and **add a score** to these combinations\n",
    "* **Flag products** that a customer has bought already **based on previous transactions**\n",
    "* Apply the **business logic** per customer\n",
    "    * \"Only recommend products a customer has not bought already\"\n",
    "    * \"Only return the top 3 products per brand\"\n",
    "\n",
    "**Why should we use Spark here?** In our setting, we have\n",
    "* 100k customers\n",
    "* 10k products\n",
    "\n",
    "When we do the cross product, we get `100 000 * 10 000 = 1 000 000 000` (1 billion)  rows. 1 billion rows times at least 3 cols (`customer_id`, `product_id`, `score`), with 64bit precision / 8 byte precision yields 24 GB of data. If we join 1 (2,3) product properties for filtering, we get 32 GB (40 GB, 48 GB) already. This is not \"big data\", but most probably the data is too big to fit into your local machine's memory.\n",
    "\n",
    "The following image shows a sketch of the workflow:\n",
    "\n",
    "![Cross Join Customers and Prodocts Workflow](../../docs/workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application at hand, we will **discover the following key lessons learned** (I sorted it according to the importance I assign to them, top to bottom):\n",
    "- **Use the Spark SQL / DataFrame API**.  \n",
    "  Especially if you have a Pandas / SQL background. This structured data API is more high level than the Resilient Distributed Dataset (RDD) API. The SQL/DataFrame API is actively developed, new features are shipped frequently.\n",
    "- **Talk to Spark using plain old SQL.**  \n",
    "  As SQL is a mature technology with a large user base, your company's analysts and  BI department might speak SQL fluently already, thus they might be able to adapt your application quickly. \n",
    "- **Spark does lazy evaluation**.  \n",
    "   When reading data with a `read` statement, we set a starting point for the **execution graph**. Nothing is done until so called **actions** trigger the processing. A list of actions (not exhaustive): `count`, `show`, `head`, `write.{parquet, json, csv, saveAsTable}`. \n",
    "- **Avoid CSV as a serialization format, use Parquet instead**.  \n",
    "  CSV comes without schema, and schema inference might take very long at initial read if the data to be read is not small.\n",
    "- **Do checkpointing frequently, either to Parquet or to Hive tables.**  \n",
    "  \"Checkpointing\" is storing of intermediate results. As Spark does lazy evaluation, you might be wondering why things take so long. But a word of caution: Don't do it too frequently, because then Spark cannot optimize across the stages of the execution graph.\n",
    "- **Only a partitioned dataset can be processed in parallel.**  \n",
    "  Partitions are always processed as a whole. By default, we can assume that **1 partition = 1 (executor) core = 1 task** holds. \n",
    "- **For an efficient cross join of two rather small tables, do repartition the left table.**  \n",
    "  Otherwise the cross join cannot be parallelized. Small tables (< 100 MB) might be read into one partition only.\n",
    "\n",
    "The lessons learned are also summarized in the following diagram:\n",
    "![Lessons Learned Overview Diagram](../../docs/lessons_learned_overview_250dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime was recorded on a Google Cloud Dataproc cluster with 2 `n1-standard-8` workers with 8 vCPUs and 30 GB RAM each. By now (July 2019), Google's managed cluster service \"Dataproc\" ships with a fully functional Jupyter installation out of the box. You can find the cluster configuration amongst this blog post's notebook in [the accompanying Github repo](https://github.com/nikhase/spark-on-gcp-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read customers, products and transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To log notebook execution time\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate a **SparkSession**. When searching StackOverflow, you might encounter `{Spark, SQL, Hive}Context` as well. In the newer versions of Spark, these contexts were all unified into the `SparkSession`. \n",
    "However, it's important to `enableHiveSupport` to be able to communicate with the existing [Hive](https://hive.apache.org/index.html) installation. We use Hive to store data in **distributed tables, i.e. the table data is stored across many (Parquet) files under the hood**. Because of that, we can read and write in parallel, thus run our jobs in less wall clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 16 ms, total: 40 ms\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This takes some time when executing the command for the first time\n",
    "ss = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define and browse the data directory on Google Cloud Storage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory in a GCP bucket. We read / write to GCP directly.\n",
    "DATADIR = \"gs://spark-intro/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://spark-intro/data/\n",
      "gs://spark-intro/data/customers100000.csv\n",
      "gs://spark-intro/data/products10000.csv\n",
      "gs://spark-intro/data/output/\n",
      "gs://spark-intro/data/transactions_csv/\n",
      "gs://spark-intro/data/transactions_pq/\n"
     ]
    }
   ],
   "source": [
    "# IPython magic to browse the blob storage\n",
    "!gsutil ls $DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(df):\n",
    "    # Helper method to avoid repeating commands\n",
    "    df.printSchema()\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read customers and products from a CSV, as CSV is still a common exchange format. Specify `option(\"header\", True)` when dealing with CSVs where the header row contains the column names. With `option(\"inferSchema\", True)`, Spark will infer the schema. We will see in the next paragraph that **schema inference can be a very expensive operation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read customers (1.5 MB) and products (350 KB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "customers = ss.read.option(\"header\", True).option(\"inferSchema\", True).csv(DATADIR + \"customers100000.csv\")\n",
    "products = ss.read.option(\"header\", True).option(\"inferSchema\", True).csv(DATADIR + \"products10000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count customers and products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- is_male: boolean (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      "\n",
      "+-------+-------+--------+\n",
      "|cust_id|is_male|location|\n",
      "+-------+-------+--------+\n",
      "|1000000|   true|       1|\n",
      "|1000001|  false|       2|\n",
      "|1000002|   true|       3|\n",
      "|1000003|  false|       4|\n",
      "|1000004|   true|       1|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- food: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- at_location: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+-------+------------+\n",
      "|product_id| food| price|  brand| at_location|\n",
      "+----------+-----+------+-------+------------+\n",
      "|     10000| true| 99.38| luxury|         [1]|\n",
      "|     10001|false|141.12| luxury|      [1, 2]|\n",
      "|     10002| true|151.15|premium|   [1, 2, 3]|\n",
      "|     10003|false| 62.31|premium|[1, 2, 3, 4]|\n",
      "|     10004| true| 92.95| luxury|         [1]|\n",
      "+----------+-----+------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(customers)\n",
    "info(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 793 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 278 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "products.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is running smoothly so far, the results come almost instantly. Could as well be a Pandas execution now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read transaction data (18 GB CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACTIONS_CSV_PATH = transactions_csv_path = DATADIR + \"transactions_csv\"\n",
    "TRANSACTIONS_PARQUET_PATH = DATADIR + \"transactions_pq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files are we going to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B          gs://spark-intro/data/transactions_csv/\n",
      "0 B          gs://spark-intro/data/transactions_csv/_SUCCESS\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00000-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00001-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00002-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00003-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00004-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00005-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00006-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00007-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00008-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00009-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00010-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00011-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00012-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00013-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00014-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00015-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00016-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00017-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00018-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00019-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00020-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00021-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00022-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00023-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00024-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00025-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00026-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00027-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00028-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00029-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00030-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00031-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00032-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00033-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00034-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00035-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00036-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.56 MiB   gs://spark-intro/data/transactions_csv/part-00037-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00038-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00039-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00040-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00041-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00042-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00043-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00044-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00045-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00046-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00047-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00048-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "371.19 MiB   gs://spark-intro/data/transactions_csv/part-00049-8d19b503-8b7a-428f-8013-3a3920e35ac4-c000.csv\n",
      "18.13 GiB    gs://spark-intro/data/transactions_csv/\n"
     ]
    }
   ],
   "source": [
    "!gsutil du -h $TRANSACTIONS_CSV_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will read 18 GB of CSVs distributed across 50 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+-------+----------+--------------------+\n",
      "|cust_id|product_id|                date|\n",
      "+-------+----------+--------------------+\n",
      "|1087831|     10000|2018-05-23T23:04:...|\n",
      "|1087831|     10001|2018-09-03T20:20:...|\n",
      "|1087831|     10002|2018-08-07T15:21:...|\n",
      "|1087831|     10003|2018-08-27T08:00:...|\n",
      "|1087831|     10004|2018-11-29T22:50:...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transactions_csv = ss.read.option(\"header\", True).csv(transactions_csv_path)\n",
    "info(transactions_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was very fast. Did we really read in all the CSV data now? \n",
    "No, `read()` does not bring the data into memory. Instead, Spark just adds the data source to the execution graph. This is Spark's **lazy evaluation**, we did not trigger an action yet. Let's trigger the action `count()`, which will scan all data in the CSVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 38.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499180000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transactions_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lesson learned: **Spark does lazy evaluation**.  \n",
    "When reading data with a `read` statement, we set a starting point for the **execution graph**. Nothing is done until so called **actions** trigger the processing. A list of actions (not exhaustive): `count`, `show`, `head`, `write.{parquet, json, csv, saveAsTable}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema is still not correct. Let's do **inferSchema**.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 8 ms, total: 48 ms\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transactions_csv = ss.read.option(\"header\", True).option(\"inferSchema\", True).csv(transactions_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+-------+----------+--------------------+\n",
      "|cust_id|product_id|                date|\n",
      "+-------+----------+--------------------+\n",
      "|1087831|     10000|2018-05-23 23:04:...|\n",
      "|1087831|     10001|2018-09-03 20:20:...|\n",
      "|1087831|     10002|2018-08-07 15:21:...|\n",
      "|1087831|     10003|2018-08-27 08:00:...|\n",
      "|1087831|     10004|2018-11-29 22:50:...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(transactions_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema inference took quite some time, because Spark needed to scan every single value in the data (!) to perform the inference.** Using parquet is the better option, the schema is stored in the metadata and I/O is way faster. However, it is very common that third parties provide data in the CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B          gs://spark-intro/data/transactions_pq/\n",
      "0 B          gs://spark-intro/data/transactions_pq/_SUCCESS\n",
      "108.74 MiB   gs://spark-intro/data/transactions_pq/part-00000-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.81 MiB   gs://spark-intro/data/transactions_pq/part-00001-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.73 MiB   gs://spark-intro/data/transactions_pq/part-00002-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.76 MiB   gs://spark-intro/data/transactions_pq/part-00003-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.69 MiB   gs://spark-intro/data/transactions_pq/part-00004-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.72 MiB   gs://spark-intro/data/transactions_pq/part-00005-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.69 MiB   gs://spark-intro/data/transactions_pq/part-00006-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.81 MiB   gs://spark-intro/data/transactions_pq/part-00007-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.81 MiB   gs://spark-intro/data/transactions_pq/part-00008-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00009-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.68 MiB   gs://spark-intro/data/transactions_pq/part-00010-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.89 MiB   gs://spark-intro/data/transactions_pq/part-00011-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.79 MiB   gs://spark-intro/data/transactions_pq/part-00012-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "109 MiB      gs://spark-intro/data/transactions_pq/part-00013-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.86 MiB   gs://spark-intro/data/transactions_pq/part-00014-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00015-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.85 MiB   gs://spark-intro/data/transactions_pq/part-00016-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.94 MiB   gs://spark-intro/data/transactions_pq/part-00017-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.81 MiB   gs://spark-intro/data/transactions_pq/part-00018-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.78 MiB   gs://spark-intro/data/transactions_pq/part-00019-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.84 MiB   gs://spark-intro/data/transactions_pq/part-00020-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.79 MiB   gs://spark-intro/data/transactions_pq/part-00021-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.83 MiB   gs://spark-intro/data/transactions_pq/part-00022-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.8 MiB    gs://spark-intro/data/transactions_pq/part-00023-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.74 MiB   gs://spark-intro/data/transactions_pq/part-00024-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.93 MiB   gs://spark-intro/data/transactions_pq/part-00025-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.83 MiB   gs://spark-intro/data/transactions_pq/part-00026-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.91 MiB   gs://spark-intro/data/transactions_pq/part-00027-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.8 MiB    gs://spark-intro/data/transactions_pq/part-00028-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.63 MiB   gs://spark-intro/data/transactions_pq/part-00029-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.77 MiB   gs://spark-intro/data/transactions_pq/part-00030-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.73 MiB   gs://spark-intro/data/transactions_pq/part-00031-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.66 MiB   gs://spark-intro/data/transactions_pq/part-00032-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00033-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.82 MiB   gs://spark-intro/data/transactions_pq/part-00034-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.8 MiB    gs://spark-intro/data/transactions_pq/part-00035-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.83 MiB   gs://spark-intro/data/transactions_pq/part-00036-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00037-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.81 MiB   gs://spark-intro/data/transactions_pq/part-00038-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.76 MiB   gs://spark-intro/data/transactions_pq/part-00039-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.74 MiB   gs://spark-intro/data/transactions_pq/part-00040-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.72 MiB   gs://spark-intro/data/transactions_pq/part-00041-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.78 MiB   gs://spark-intro/data/transactions_pq/part-00042-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.73 MiB   gs://spark-intro/data/transactions_pq/part-00043-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.76 MiB   gs://spark-intro/data/transactions_pq/part-00044-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00045-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.75 MiB   gs://spark-intro/data/transactions_pq/part-00046-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.77 MiB   gs://spark-intro/data/transactions_pq/part-00047-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.79 MiB   gs://spark-intro/data/transactions_pq/part-00048-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "108.66 MiB   gs://spark-intro/data/transactions_pq/part-00049-65356b68-a1e4-4cab-ac79-798c4897781c-c000.snappy.parquet\n",
      "5.31 GiB     gs://spark-intro/data/transactions_pq/\n"
     ]
    }
   ],
   "source": [
    "!gsutil du -h $TRANSACTIONS_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `parquet` consume less space than the CSV files (to be fair, the files are also compressed with `snappy` here. You could also apply `snappy` compression on CSV, but the magnitude of difference would stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+-------+----------+--------------------+\n",
      "|cust_id|product_id|                date|\n",
      "+-------+----------+--------------------+\n",
      "|1087831|     10000|2018-05-23 23:04:...|\n",
      "|1087831|     10001|2018-09-03 20:20:...|\n",
      "|1087831|     10002|2018-08-07 15:21:...|\n",
      "|1087831|     10003|2018-08-27 08:00:...|\n",
      "|1087831|     10004|2018-11-29 22:50:...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 4.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transactions_parquet = ss.read.parquet(TRANSACTIONS_PARQUET_PATH)\n",
    "info(transactions_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499180000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transactions_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the result is the same, but we needed way less wall clock time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lesson learned: **Avoid CSV as a serialization format, use Parquet instead**.  \n",
    "CSV comes without schema, and schema inference might take very long at initial read if the data to be read is not small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the transactions loaded from Parquet from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the underlying Hive installation and data using the `catalog` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available tables\n",
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By registering the transactions as a temporary view (`TempView`), we can now write **SQL queries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView(\"transactions_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='transactions_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|499180000|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\n",
    "\"\"\"\n",
    "SELECT COUNT(*) FROM transactions_view\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the dataframe as a Hive table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transactions.write.mode(\"overwrite\").saveAsTable(\"transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we stored a table, we see that the data is stored in compressed parquet files (snappy), each roughly 100 MB in size. We get the total storage footprint on HDFS using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        /user/hive/warehouse/transactions/_SUCCESS\n",
      "109.0 M  /user/hive/warehouse/transactions/part-00000-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00001-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00002-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00003-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00004-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00005-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.9 M  /user/hive/warehouse/transactions/part-00006-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00007-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00008-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00009-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00010-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00011-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00012-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00013-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00014-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00015-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00016-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00017-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00018-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00019-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00020-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00021-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00022-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00023-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00024-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00025-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00026-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00027-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00028-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00029-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00030-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00031-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00032-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.8 M  /user/hive/warehouse/transactions/part-00033-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00034-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00035-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00036-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00037-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00038-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00039-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00040-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00041-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00042-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00043-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00044-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00045-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00046-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00047-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.7 M  /user/hive/warehouse/transactions/part-00048-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "108.6 M  /user/hive/warehouse/transactions/part-00049-40bd7799-62c7-4a84-81b5-30ff572fae67-c000.snappy.parquet\n",
      "5.3 G  /user/hive/warehouse/transactions\n"
     ]
    }
   ],
   "source": [
    "!sudo -u hdfs hadoop fs -du -h /user/hive/warehouse/transactions && sudo -u hdfs hadoop fs -du -s -h /user/hive/warehouse/transactions  # list all && show sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the table is **persisted on the disks of the cluster**, not in GCS. This means it will be gone once the cluster is shut down. This has pros and cons:\n",
    "\n",
    "`(+)` Store intermediate data on the cluster's disks is (normally) faster and needs less bandwidth than storing on GCS  \n",
    "`(+)` Data is lost after cluster deletion (i.e. you are not polluting your long time storage)  \n",
    "\n",
    "`(-)` Data is lost after cluster deletion (bad if you forgot to export it to long term storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's query the Hive table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SparkSession.sql(...) statement returns a DataFrame\n",
    "df = ss.sql(\n",
    "\"\"\"\n",
    "SELECT * FROM transactions LIMIT 100\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+-------+----------+--------------------+\n",
      "|cust_id|product_id|                date|\n",
      "+-------+----------+--------------------+\n",
      "|1004545|     10000|2018-12-01 03:52:...|\n",
      "|1004545|     10001|2018-02-18 03:23:...|\n",
      "|1004545|     10002|2018-08-05 12:24:...|\n",
      "|1004545|     10003|2018-08-29 16:23:...|\n",
      "|1004545|     10004|2018-10-12 15:15:...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cust_id_count = ss.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "cust_id, count(1) AS product_count\n",
    "FROM transactions\n",
    "GROUP BY cust_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n",
      "+-------+-------------+\n",
      "|cust_id|product_count|\n",
      "+-------+-------------+\n",
      "|1099528|        10000|\n",
      "|1047979|        10000|\n",
      "|1082028|        10000|\n",
      "|1037426|        10000|\n",
      "|1081474|        10000|\n",
      "+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(sql_cust_id_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the cross join of customers and products and add a score to these combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing a cross join, we get a list of all possible customer / product combinations. You then calculate a recommender score in order to get the relevant recommendations. In this post, we use a dummy random score, as recommender scores are out of scope at the moment.\n",
    "In order to understand how to do a cross join efficiently, we need to **talk about partitions in Spark** shortly. In general, you can always assume the following defaults: **1 partition = 1 (executor) core = 1 task**\n",
    "\n",
    "Of course, these can be adjusted. For example, in the current `2 x n1-standard-8` cluster setting, Spark allocated the resources as follows:\n",
    "* **2 workers with 8 cores each** yield\n",
    "* **3 executors with 4 cores each, i.e. 12 cores in total** (so 4 cores are not used for data crunching) yield\n",
    "* **12 partitions** that can be processed in parallel (1 partition per core) \n",
    "\n",
    "You can think of executors as Java Virtual Machines (JVMs). Master nodes don't do the number crunching, they are occupied with scheduling and organization.\n",
    "If your dataset is in 1 partition only, only 1 core can read from it and the rest will be idle. No parallelization is happening, that's bad for performance. To circumvent that, Spark sometomes performs an internal **repartition()**, which creates **200 partitions** by default. Thus 200 partitions is a sensible default we can use for repartitioning our small dataset explicitly. Let's check current partition count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1 partition for the customer data and 1 partition for the products. How many partitions do we have for the cross join result with random scores added? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_unpartitioned = (customers.\n",
    "                        crossJoin(\n",
    "                            products).\n",
    "                        select([\"cust_id\", \"product_id\"]).  # select IDs only, not the property columns\n",
    "                        withColumn(\"score\",sqlf.rand()))  # Add a column with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_unpartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does the execution take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores_unpartitioned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the repartitioned result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_repartitioned = (customers.repartition(200, \"cust_id\").\n",
    "                        crossJoin(\n",
    "                            products).\n",
    "                        select([\"cust_id\", \"product_id\"]).\n",
    "                        withColumn(\"score\",sqlf.rand()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores_repartitioned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have more partitions now, Spark was able to exploit parallelization. By now, let's state these two lessons learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lesson learned: **Only a partitioned dataset can be processed in parallel.**  \n",
    "  Partitions are always processed as a whole. By default, we can assume that **1 partition = 1 (executor) core = 1 task** holds. \n",
    "\n",
    "> ### Lesson learned: **For an efficient cross join of two rather small tables, do repartition the left table.**  \n",
    "  Otherwise the cross join cannot be parallelized. Small tables (< 100 MB) might be read into one partition only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do **checkpointing**  now: Store the results to parquet and read again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 16 ms, total: 36 ms\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_repartitioned.write.mode(\"overwrite\").parquet(DATADIR + \"output/scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = ss.read.parquet(DATADIR + \"output/scores\")\n",
    "scores.createOrReplaceTempView(\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This count is much faster than the one before, because we now start with the materialized cross join in the Hive table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lesson learned: **Do checkpointing frequently, either to Parquet or to Hive tables.**  \n",
    "\"Checkpointing\" is storing of intermediate results. As Spark does lazy evaluation, you might be wondering why things take so long. But a word of caution: Don't do it too frequently, because then Spark cannot optimize across the stages of the execution graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag products that a customer has bought already based on previous transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the flagging using SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_purchasing_history = ss.sql(\"\"\"\n",
    "SELECT cust_id, product_id, TRUE AS has_bought\n",
    "FROM transactions\n",
    "GROUP BY cust_id, product_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- has_bought: boolean (nullable = false)\n",
      "\n",
      "+-------+----------+----------+\n",
      "|cust_id|product_id|has_bought|\n",
      "+-------+----------+----------+\n",
      "|1086994|     10252|      true|\n",
      "|1086994|     10358|      true|\n",
      "|1086994|     10373|      true|\n",
      "|1086994|     10414|      true|\n",
      "|1086994|     10582|      true|\n",
      "+-------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(customer_purchasing_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Do checkpointing\n",
    "customer_purchasing_history.write.mode(\"overwrite\").saveAsTable(\"customer_purchasing_history\")\n",
    "customer_purchasing_history = ss.read.table(\"customer_purchasing_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- has_bought: boolean (nullable = true)\n",
      "\n",
      "+-------+----------+----------+\n",
      "|cust_id|product_id|has_bought|\n",
      "+-------+----------+----------+\n",
      "|1004545|     10134|      true|\n",
      "|1004545|     10704|      true|\n",
      "|1004545|     10915|      true|\n",
      "|1004545|     10928|      true|\n",
      "|1004545|     11019|      true|\n",
      "+-------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(customer_purchasing_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the business logic per customer\n",
    "\n",
    "We want to apply the following business rules:\n",
    "\n",
    "1. \"Only recommend products a customer has not bought already\"\n",
    "2. \"Only return the top 3 products per brand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the purchasing history to the scores in order to be able to apply the business rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_w_purchasing_history = ss.sql(\n",
    "\"\"\"\n",
    "SELECT s.cust_id, \n",
    "       s.product_id, \n",
    "       s.score,\n",
    "       if(isnull(c.has_bought), FALSE, c.has_bought) AS has_bought\n",
    "FROM scores s\n",
    "FULL JOIN customer_purchasing_history c  -- We need to do a full / outer join here\n",
    "ON s.cust_id = c.cust_id AND s.product_id = c.product_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SQL queries / substrings for the filter application. This way, we can read the SQL strings from some config file (for example YAML) and directly apply them to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary of business rules shows all applied business rules at a glance\n",
    "business_rules = {\n",
    "    \"filter1\": \"has_bought = FALSE\",\n",
    "    \"filter2\": (\"RANK() OVER (PARTITION BY cust_id , brand ORDER BY score DESC) AS rank\", \n",
    "                \"rank <= 3\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Lesson learned: **Talk to Spark using plain old SQL.**  \n",
    "As SQL is a mature technology with a large user base, your company's analysts and  BI department might speak SQL fluently already, thus they might be able to adapt your application quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Rule 1: Only recommend products a customer has not bought already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- has_bought: boolean (nullable = true)\n",
      "\n",
      "+-------+----------+-------------------+----------+\n",
      "|cust_id|product_id|              score|has_bought|\n",
      "+-------+----------+-------------------+----------+\n",
      "|1000001|     10196| 0.5560191718986174|     false|\n",
      "|1000001|     10445|0.05978026533889658|     false|\n",
      "|1000001|     11040| 0.7903298929818295|     false|\n",
      "|1000001|     11118| 0.3121355030520022|     false|\n",
      "|1000001|     11355| 0.5067529105046227|     false|\n",
      "+-------+----------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 12 ms, sys: 16 ms, total: 28 ms\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filter1 = business_rules[\"filter1\"]\n",
    "info(scores_w_purchasing_history.where(filter1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 20 ms, total: 52 ms\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_w_purchasing_history.where(filter1).write.mode(\"overwrite\").saveAsTable(\"scores_filter1\")\n",
    "scores_filter1 = ss.read.table(\"scores_filter1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 350 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500820000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores_filter1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Rule 2: Only return the top 3 products per brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to join the product properties to get the field \"brand\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_filter1_w_product_properties = scores_filter1.join(products, on=[\"product_id\"])  # Here we use the dataframe API instead of the SQL API\n",
    "scores_filter1_w_product_properties.createOrReplaceTempView(\"scores_filter1_w_product_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- has_bought: boolean (nullable = true)\n",
      " |-- food: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- at_location: string (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+----------+-----+------+------+------------+\n",
      "|product_id|cust_id|               score|has_bought| food| price| brand| at_location|\n",
      "+----------+-------+--------------------+----------+-----+------+------+------------+\n",
      "|     10041|1000001|  0.5362723494069369|     false|false| 75.02|luxury|      [1, 2]|\n",
      "|     10921|1000001|  0.6362841811765275|     false|false| 98.95| basic|      [1, 2]|\n",
      "|     11253|1000001|0.016567943451530565|     false|false|119.49| basic|      [1, 2]|\n",
      "|     11368|1000001|0.005697748287225246|     false| true| 66.49| basic|         [1]|\n",
      "|     11551|1000001|  0.9909017982612867|     false|false| 57.47|luxury|[1, 2, 3, 4]|\n",
      "+----------+-------+--------------------+----------+-----+------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(scores_filter1_w_product_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return the top 3 products per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking1 = business_rules[\"filter2\"][0]  # could be input by the user\n",
    "filter2 = business_rules[\"filter2\"][1]  # also input by the user\n",
    "sql_query = \"\"\"\n",
    "WITH ranks AS (\n",
    "SELECT \n",
    "    s.cust_id, \n",
    "    s.product_id,\n",
    "    s.score,\n",
    "    s.brand,\n",
    "    {ranking}\n",
    "FROM scores_filter1_w_product_properties s\n",
    ")\n",
    "SELECT * FROM ranks WHERE {cond}\n",
    "\"\"\".format(ranking=ranking1, cond=filter2)\n",
    "top_recommendations = ss.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 8 ms, total: 32 ms\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_recommendations.write.mode(\"overwrite\").parquet(DATADIR + \"output/top_recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-------+----+\n",
      "|cust_id|product_id|             score|  brand|rank|\n",
      "+-------+----------+------------------+-------+----+\n",
      "|1000224|     15541|0.9992690565661719|  basic|   1|\n",
      "|1000224|     19826|0.9988429707324326|  basic|   2|\n",
      "|1000224|     11270|0.9983119418727431|  basic|   3|\n",
      "|1000272|     16146|0.9999730200682578| luxury|   1|\n",
      "|1000272|     12988|0.9994097429171147| luxury|   2|\n",
      "|1000272|     19130|0.9993081117794463| luxury|   3|\n",
      "|1000386|     19329|  0.99926192778321|premium|   1|\n",
      "|1000386|     11914|0.9992004365501195|premium|   2|\n",
      "|1000386|     11968|0.9988237313923024|premium|   3|\n",
      "|1000674|     14053|0.9989064792809064|premium|   1|\n",
      "|1000674|     16968|0.9987840019849697|premium|   2|\n",
      "|1000674|     10211|0.9986741256942666|premium|   3|\n",
      "|1000984|     18821|0.9999358092667335|  basic|   1|\n",
      "|1000984|     12681|0.9997864456337203|  basic|   2|\n",
      "|1000984|     17815|0.9997322494495897|  basic|   3|\n",
      "|1001039|     16676|0.9997762330760541|premium|   1|\n",
      "|1001039|     15978|0.9997315139761564|premium|   2|\n",
      "|1001039|     17666|0.9988470519494469|premium|   3|\n",
      "|1001213|     16441|0.9990950246587061| luxury|   1|\n",
      "|1001213|     12962|0.9981221579176125| luxury|   2|\n",
      "+-------+----------+------------------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_recommendations = ss.read.parquet(DATADIR + \"output/top_recommendations\")\n",
    "top_recommendations.createOrReplaceTempView(\"top_recommendations\")\n",
    "top_recommendations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have our result. Some sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_count = ss.sql(\"\"\"\n",
    "SELECT cust_id, brand, COUNT(product_id) AS prod_count\n",
    "FROM top_recommendations\n",
    "GROUP BY cust_id, brand\n",
    "\"\"\")\n",
    "brand_count.createOrReplaceTempView(\"brand_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|min(prod_count)|max(prod_count)|\n",
      "+---------------+---------------+\n",
      "|              3|              3|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "SELECT MIN(prod_count), MAX(prod_count) FROM brand_count\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. At last, I want to mention the very first lesson learned from the top of the post, as we are using it all the time:\n",
    "\n",
    "> ### Lesson learned: **Use the Spark SQL / DataFrame API**.  \n",
    "Especially if you have a Pandas / SQL background. This structured data API is more high level than the Resilient Distributed Dataset (RDD) API. The SQL/DataFrame API is actively developed, new features are shipped frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution took 1344.86s\n"
     ]
    }
   ],
   "source": [
    "elapsed = time.time() - start\n",
    "print(\"Notebook execution took {:.2f}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was this post useful for you? Do you disagree at some point? Please leave a comment and share your thoughts! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuts down the IPython kernel of the Jupyter notebook\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The End*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
